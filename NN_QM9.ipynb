{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/0f/5157e6b153b3d4a70dc5fbe2ab6f209604197590f387f03177b7a249ac60/lightgbm-2.2.3-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in ./anaconda3/lib/python3.7/site-packages (from lightgbm) (0.21.2)\n",
      "Requirement already satisfied: scipy in ./anaconda3/lib/python3.7/site-packages (from lightgbm) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.7/site-packages (from lightgbm) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in ./anaconda3/lib/python3.7/site-packages (from scikit-learn->lightgbm) (0.13.2)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --user lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\n",
      "\u001b[K     |████████████████████████████████| 109.3MB 79.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/fd/60ce148d8e4205bdf6da4ffec31348fd33f710c20a882b44319d54fd51ae/protobuf-3.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 50.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (1.16.4)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.4)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/27/1f908ebb99c8d48a5ba4eb9d7997f5633b920d98fe712f67aaa0663f1307/grpcio-1.23.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 52.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./anaconda3/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Building wheels for collected packages: termcolor, gast, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/amitkumarj441/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/amitkumarj441/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/amitkumarj441/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "Successfully built termcolor gast absl-py\n",
      "Installing collected packages: protobuf, tensorflow-estimator, markdown, grpcio, absl-py, tensorboard, termcolor, gast, keras-applications, keras-preprocessing, google-pasta, astor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.23.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 protobuf-3.9.1 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0\n",
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.7/site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in ./.local/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in ./.local/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.3.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --user tensorflow\n",
    "!pip install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['structures.zip', 'scalar_coupling_contributions.csv', 'structures.csv', 'potential_energy.csv', 'dipole_moments.csv', 'magnetic_shielding_tensors.csv', 'sample_submission.csv', 'test.csv', 'mulliken_charges.csv', 'data.covs.pickle', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/home/amitkumarj441/input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/amitkumarj441/input'\n",
    "SUBMISSIONS_PATH = './'\n",
    "# use atomic numbers to recode atomic names\n",
    "ATOMIC_NUMBERS = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('/home/amitkumarj441/input/data.covs.pickle')\n",
    "data_train = data.loc[~data['scalar_coupling_constant'].isnull()].reset_index(drop=True)\n",
    "data_test = data.loc[data['scalar_coupling_constant'].isnull()].reset_index(drop=True)\n",
    "data_train=data_train.sort_values(by='id',ascending=True).reset_index()\n",
    "data_train=data_train.drop(['index','molecule_name','atom_index_0','atom_index_1','type','scalar_coupling_constant'],axis=1)\n",
    "data_test=data_test.sort_values(by='id',ascending=True).reset_index()\n",
    "data_test=data_test.drop(['index','molecule_name','atom_index_0','atom_index_1','type','scalar_coupling_constant'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 644 /home/amitkumarj441/input/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 644 /home/amitkumarj441/input/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 644 /home/amitkumarj441/input/*csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 644 /home/amitkumarj441/input/structures.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index_0': 'int8',\n",
    "    'atom_index_1': 'int8',\n",
    "    'type': 'category',\n",
    "    'scalar_coupling_constant': 'float32'\n",
    "}\n",
    "train_csv = pd.read_csv(f'{DATA_PATH}/train.csv', index_col='id', dtype=train_dtypes)\n",
    "train_csv['molecule_index'] = train_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "train_csv = train_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\n",
    "\n",
    "train_csv=train_csv.reset_index()\n",
    "train_csv=pd.merge(train_csv,data_train,on='id',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = pd.read_csv(f'{DATA_PATH}/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index atom         x         y         z\n",
       "0  dsgdb9nsd_000001  0           C   -0.012698  1.085804  0.008001\n",
       "1  dsgdb9nsd_000001  1           H    0.002150 -0.006031  0.001976\n",
       "2  dsgdb9nsd_000001  2           H    1.011731  1.463751  0.000277\n",
       "3  dsgdb9nsd_000001  3           H   -0.540815  1.447527 -0.876644\n",
       "4  dsgdb9nsd_000001  4           H   -0.523814  1.437933  0.906397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_csv = pd.read_csv(f'{DATA_PATH}/test.csv', index_col='id', dtype=train_dtypes)\n",
    "test_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "test_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "\n",
    "test_csv=test_csv.reset_index()\n",
    "test_csv=pd.merge(test_csv,data_test,on='id',how='right')\n",
    "\n",
    "structures_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index': 'int8',\n",
    "    'atom': 'category',\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}\n",
    "structures_csv = pd.read_csv(f'{DATA_PATH}/structures.csv', dtype=structures_dtypes)\n",
    "display(structures_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "structures_csv = structures_csv[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\n",
    "structures_csv['atom'] = structures_csv['atom'].replace(ATOMIC_NUMBERS).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n",
    "    return base, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coordinates(base, structures, index):\n",
    "    df = pd.merge(base, structures, how='inner',\n",
    "                    left_on=['molecule_index', f'atom_index_{index}'],\n",
    "                    right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner',\n",
    "                  on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left',\n",
    "                  left_on=['molecule_index'],\n",
    "                  right_on=['molecule_index'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_center(df):\n",
    "    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n",
    "    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n",
    "    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n",
    "\n",
    "def add_distance_to_center(df):\n",
    "    df['d_c'] = ((\n",
    "        (df['x_c'] - df['x'])**np.float32(2) +\n",
    "        (df['y_c'] - df['y'])**np.float32(2) + \n",
    "        (df['z_c'] - df['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    base = add_coordinates(base, structures, 0)\n",
    "    base = add_coordinates(base, structures, 1)\n",
    "    base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    atoms = base.drop('id', axis=1).copy()\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n",
    "        \n",
    "    add_center(atoms)\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "\n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "\n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "   \n",
    "    \n",
    "    # downcast back to int8\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "   \n",
    "    \n",
    "    full = add_atoms(base, atoms)\n",
    "    \n",
    "    add_distances(full)\n",
    "    \n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = ['rc_A', 'rc_B', 'rc_C', 'mu', 'alpha',\n",
    "       'homo', 'lumo', 'gap', 'r2', 'zpve', 'U0', 'U', 'H', 'G', 'Cv',\n",
    "       'freqs_min', 'freqs_max', 'freqs_mean', 'linear', 'mulliken_min',\n",
    "       'mulliken_max', 'mulliken_mean', 'mulliken_atom_0', 'mulliken_atom_1']\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "    \n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "    return df[labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(2048, activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"linear\")(x)  \n",
    "   # out1 = Dense(2, activation=\"linear\")(x)#mulliken charge 2\n",
    "   # out2 = Dense(6, activation=\"linear\")(x)#tensor 6(xx,yy,zz)\n",
    "   # out3 = Dense(12, activation=\"linear\")(x)#tensor 12(others) \n",
    "   # out4 = Dense(1, activation=\"linear\")(x)#scalar_coupling_constant \n",
    "    #model = Model(inputs=inp, outputs=[out,out1,out2,out3,out4])\n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of [1JHC, 2JHH, 1JHN, 2JHN, 2JHC, 3JHH, 3JHC, 3JHN]\n",
      "Categories (8, object): [1JHC, 2JHH, 1JHN, 2JHN, 2JHC, 3JHH, 3JHC, 3JHN] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 23:25:18.451591 139679031473984 deprecation_wrapper.py:119] From /home/amitkumarj441/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0826 23:25:18.452834 139679031473984 deprecation_wrapper.py:119] From /home/amitkumarj441/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0826 23:25:18.456532 139679031473984 deprecation_wrapper.py:119] From /home/amitkumarj441/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0826 23:25:18.526583 139679031473984 deprecation_wrapper.py:119] From /home/amitkumarj441/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0826 23:25:18.872673 139679031473984 deprecation_wrapper.py:119] From /home/amitkumarj441/.local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0826 23:25:19.458922 139679031473984 deprecation_wrapper.py:119] From /home/amitkumarj441/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 638474 samples, validate on 70942 samples\n",
      "Epoch 1/1000\n",
      "638474/638474 [==============================] - 84s 131us/step - loss: 56.8365 - val_loss: 42.7093\n",
      "Epoch 2/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 1.8159 - val_loss: 1.9943\n",
      "Epoch 3/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 1.5065 - val_loss: 1.9830\n",
      "Epoch 4/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.3581 - val_loss: 1.5589\n",
      "Epoch 5/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.2907 - val_loss: 1.3101\n",
      "Epoch 6/1000\n",
      "638474/638474 [==============================] - 81s 127us/step - loss: 1.2396 - val_loss: 1.3755\n",
      "Epoch 7/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 1.1960 - val_loss: 1.3039\n",
      "Epoch 8/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 1.1515 - val_loss: 1.2351\n",
      "Epoch 9/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.1166 - val_loss: 1.2653\n",
      "Epoch 10/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.0918 - val_loss: 1.1741\n",
      "Epoch 11/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.0815 - val_loss: 1.4138\n",
      "Epoch 12/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.0440 - val_loss: 1.1291\n",
      "Epoch 13/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.0262 - val_loss: 1.0974\n",
      "Epoch 14/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 1.0142 - val_loss: 1.3179\n",
      "Epoch 15/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 0.9855 - val_loss: 0.9966\n",
      "Epoch 16/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.9793 - val_loss: 1.1214\n",
      "Epoch 17/1000\n",
      "638474/638474 [==============================] - 83s 129us/step - loss: 0.9717 - val_loss: 1.1347\n",
      "Epoch 18/1000\n",
      "638474/638474 [==============================] - 83s 129us/step - loss: 0.9546 - val_loss: 0.9721\n",
      "Epoch 19/1000\n",
      "638474/638474 [==============================] - 83s 129us/step - loss: 0.9347 - val_loss: 0.9245\n",
      "Epoch 20/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8980 - val_loss: 0.8827\n",
      "Epoch 21/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.9012 - val_loss: 0.8901\n",
      "Epoch 22/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8899 - val_loss: 1.1880\n",
      "Epoch 23/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8809 - val_loss: 1.2642\n",
      "Epoch 24/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8779 - val_loss: 0.9325\n",
      "Epoch 25/1000\n",
      "638474/638474 [==============================] - 83s 129us/step - loss: 0.8521 - val_loss: 0.9429\n",
      "Epoch 26/1000\n",
      "638474/638474 [==============================] - 83s 129us/step - loss: 0.8620 - val_loss: 0.8656\n",
      "Epoch 27/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8473 - val_loss: 0.9721\n",
      "Epoch 28/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8311 - val_loss: 0.8641\n",
      "Epoch 29/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8320 - val_loss: 1.0942\n",
      "Epoch 30/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8140 - val_loss: 1.0303\n",
      "Epoch 31/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8040 - val_loss: 0.8647\n",
      "Epoch 32/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7985 - val_loss: 0.8442\n",
      "Epoch 33/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.8113 - val_loss: 0.8407\n",
      "Epoch 34/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7936 - val_loss: 0.7730\n",
      "Epoch 35/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 0.7781 - val_loss: 0.8831\n",
      "Epoch 36/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7854 - val_loss: 0.8393\n",
      "Epoch 37/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7643 - val_loss: 0.8435\n",
      "Epoch 38/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7594 - val_loss: 0.9869\n",
      "Epoch 39/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7446 - val_loss: 0.7935\n",
      "Epoch 40/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7569 - val_loss: 0.8480\n",
      "Epoch 41/1000\n",
      "638474/638474 [==============================] - 82s 129us/step - loss: 0.7485 - val_loss: 0.8279\n",
      "Epoch 42/1000\n",
      "638474/638474 [==============================] - 82s 128us/step - loss: 0.7227 - val_loss: 1.0180\n",
      "Epoch 43/1000\n",
      "157696/638474 [======>.......................] - ETA: 59s - loss: 0.7284 "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "mol_types=train_csv[\"type\"].unique()\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 1000\n",
    "verbose = 1\n",
    "batch_size = 2048\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain =True\n",
    "\n",
    "start_time=datetime.now()\n",
    "test_prediction=np.zeros(len(test_csv))\n",
    "input_features = ['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7',\n",
    "       'atom_8','atom_9', 'atom_10','d_1_0', 'd_2_0', 'd_2_1', 'd_3_0',\n",
    "       'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0',\n",
    "       'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3',\n",
    "       'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1', 'd_8_2',\n",
    "       'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3', 'd_10_0', 'd_10_1', 'd_10_2',\n",
    "       'd_10_3','rc_A', 'rc_B', 'rc_C', 'mu', 'alpha',\n",
    "       'homo', 'lumo', 'gap', 'r2', 'zpve', 'U0', 'U', 'H', 'G', 'Cv',\n",
    "       'freqs_min', 'freqs_max', 'freqs_mean', 'linear', 'mulliken_min',\n",
    "       'mulliken_max', 'mulliken_mean', 'mulliken_atom_0', 'mulliken_atom_1']\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "\n",
    "    model_name_wrt = ('molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    full = build_couple_dataframe(train_csv, structures_csv, mol_type, n_atoms=11)\n",
    "    full2 = build_couple_dataframe(test_csv, structures_csv, mol_type, n_atoms=11)\n",
    "    df_train_ = take_n_atoms(full, 11)\n",
    "    df_test_ = take_n_atoms(full2, 11)\n",
    "    df_train_  = df_train_.fillna(0)\n",
    "    df_test_  = df_test_.fillna(0)\n",
    "    \n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))   \n",
    "    #input_data=StandardScaler().fit_transform(df_train_.loc[:,input_features])\n",
    "    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n",
    "\n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.1)\n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_target=target_data[train_index]\n",
    "    cv_target=target_data[cv_index]\n",
    "    train_input=input_data[train_index]\n",
    "    cv_input=input_data[cv_index]\n",
    "    test_input=input_data[len(df_train_):,:]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(train_input.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam())#, metrics=[auc])\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=40,verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=30, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "    history = nn_model.fit(train_input,[train_target], \n",
    "            validation_data=(cv_input,[cv_target]), \n",
    "            callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "    accuracy=np.mean(np.abs(cv_target-cv_predict[:,0]))\n",
    "    print(np.log(accuracy))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[test_csv[\"type\"]==mol_type]=test_predict[:,0]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i=0\n",
    "for mol_type in mol_types: \n",
    "    print(mol_type,\": cv score is \",cv_score[i])\n",
    "    i+=1\n",
    "print(\"total cv score is\",cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submits(predictions):\n",
    " \n",
    "    submit[\"scalar_coupling_constant\"] = predictions\n",
    "    submit.to_csv(\"submission.csv\", index=False)\n",
    "submits(test_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
